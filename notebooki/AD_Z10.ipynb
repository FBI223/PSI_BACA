{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2a5066",
   "metadata": {},
   "source": [
    "# Laboratorium nr 10"
   ]
  },
  {
   "cell_type": "code",
   "id": "02ec6776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:21:26.153203Z",
     "start_time": "2025-05-18T23:21:25.510736Z"
    }
   },
   "source": [
    "# Importy niezbędnych paczek\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d74fd154",
   "metadata": {},
   "source": [
    "# Ensemble i Soft Voting\n",
    "\n",
    "## Czym są metody ensemble?\n",
    "\n",
    "Metody ensemble (ang. ensemble methods) to techniki w uczeniu maszynowym, które łączą przewidywania wielu modeli (tzw. modeli bazowych) w celu uzyskania lepszej dokładności i stabilności predykcji niż pojedynczy model. Ideą jest wykorzystanie różnorodności modeli, aby zredukować błędy wynikające z nadmiernego dopasowania (overfitting), wariancji lub obciążenia (bias).\n",
    "\n",
    "### Główne typy metod ensemble:\n",
    "1. **Bagging** (Bootstrap Aggregating): Modele trenuje się na różnych podzbiorach danych (losowo wybieranych z powtórzeniami), a wyniki są agregowane, np. przez średnią (dla regresji) lub głosowanie większościowe (dla klasyfikacji). Przykład: Random Forest.\n",
    "2. **Boosting**: Modele trenuje się sekwencyjnie, gdzie każdy kolejny model skupia się na poprawianiu błędów poprzednich. Przykład: Gradient Boosting, AdaBoost.\n",
    "3. **Stacking**: Wyniki wielu modeli są łączone za pomocą meta-modelu, który uczy się, jak najlepiej ważyć przewidywania modeli bazowych.\n",
    "4. **Voting**: Przewidywania wielu modeli są łączone przez głosowanie (dla klasyfikacji) lub średnią (dla regresji). Wyróżniamy **hard voting** i **soft voting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Soft Voting - Szczegóły i Matematyka\n",
    "\n",
    "### Czym jest soft voting?\n",
    "\n",
    "Soft voting to technika ensemble dla problemów klasyfikacji, w której przewidywania modeli bazowych są łączone na podstawie **prawdopodobieństw przynależności do klas**. Każdy model bazowy zwraca prawdopodobieństwa dla każdej klasy, a końcowa predykcja jest obliczana jako średnia ważona tych prawdopodobieństw, po czym wybierana jest klasa o najwyższym średnim prawdopodobieństwie.\n",
    "\n",
    "W przeciwieństwie do **hard voting**, gdzie każdy model oddaje jeden \"głos\" na klasę (wynik jest liczony jako większość głosów), soft voting uwzględnia pewność modelu wyrażoną w prawdopodobieństwach, co zazwyczaj prowadzi do lepszych wyników.\n",
    "\n",
    "### Formalny opis matematyczny\n",
    "\n",
    "Załóżmy, że mamy:\n",
    "- $ M $ modeli bazowych (np. drzewa decyzyjne, SVM, sieci neuronowe).\n",
    "- $ K $ klas, które przewidujemy (np. $ K = 3 $ dla klas: \"kot\", \"pies\", \"ptak\").\n",
    "- Dla każdego modelu $ m $ (gdzie $ m = 1, 2, \\dots, M $) i dla każdej próbki $ x $, model zwraca wektor prawdopodobieństw przynależności do klas:\n",
    "  $$\n",
    "  P_m(x) = [p_{m,1}(x), p_{m,2}(x), \\dots, p_{m,K}(x)],\n",
    "  $$\n",
    "  gdzie $ p_{m,k}(x) $ to prawdopodobieństwo, że próbka $ x $ należy do klasy $ k $ według modelu $ m $, oraz $ \\sum_{k=1}^K p_{m,k}(x) = 1 $.\n",
    "\n",
    "#### Krok 1: Obliczenie średnich prawdopodobieństw\n",
    "Dla każdej klasy $ k $, obliczamy średnie prawdopodobieństwo na podstawie wszystkich modeli:\n",
    "$$\n",
    "\\bar{p}_k(x) = \\frac{1}{M} \\sum_{m=1}^M p_{m,k}(x).\n",
    "$$\n",
    "Jeśli modele mają różne wagi $ w_m $ (gdzie $ \\sum_{m=1}^M w_m = 1 $), to:\n",
    "$$\n",
    "\\bar{p}_k(x) = \\sum_{m=1}^M w_m \\cdot p_{m,k}(x).\n",
    "$$\n",
    "\n",
    "#### Krok 2: Wybór klasy\n",
    "Końcowa predykcja to klasa $ k^* $, która ma najwyższe średnie prawdopodobieństwo:\n",
    "$$\n",
    "k^* = \\arg\\max_{k \\in \\{1, 2, \\dots, K\\}} \\bar{p}_k(x).\n",
    "$$\n",
    "\n",
    "### Przykład\n",
    "Załóżmy, że mamy 3 modele ($ M = 3 $) i 2 klasy ($ K = 2 $, np. \"pozytywna\" i \"negatywna\"). Dla próbki $ x $, modele zwracają następujące prawdopodobieństwa:\n",
    "\n",
    "- Model 1: $ P_1(x) = [0.9, 0.1] $ (90% na klasę pozytywną, 10% na negatywną).\n",
    "- Model 2: $ P_2(x) = [0.7, 0.3] $.\n",
    "- Model 3: $ P_3(x) = [0.6, 0.4] $.\n",
    "\n",
    "Obliczamy średnie prawdopodobieństwa dla każdej klasy:\n",
    "$$\n",
    "\\bar{p}_1(x) = \\frac{0.9 + 0.7 + 0.6}{3} = 0.733,\n",
    "$$\n",
    "$$\n",
    "\\bar{p}_2(x) = \\frac{0.1 + 0.3 + 0.4}{3} = 0.267.\n",
    "$$\n",
    "Klasa o najwyższym prawdopodobieństwie to klasa 1 (pozytywna), więc $ k^* = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "## Zalety i wady soft voting\n",
    "\n",
    "### Zalety:\n",
    "- Uwzględnia pewność modeli, co prowadzi do bardziej wyważonych decyzji.\n",
    "- Często daje lepsze wyniki niż hard voting, szczególnie gdy modele są dobrze skalibrowane.\n",
    "- Elastyczność w stosowaniu wag dla różnych modeli.\n",
    "\n",
    "### Wady:\n",
    "- Wymaga, aby modele zwracały dobrze skalibrowane prawdopodobieństwa.\n",
    "- Większa złożoność obliczeniowa w porównaniu do hard voting.\n",
    "- Może być mniej skuteczne, jeśli modele są bardzo różne pod względem jakości predykcji.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4ad00",
   "metadata": {},
   "source": [
    "### Zadanie nr 1 (5 pkt)\n",
    "Zaimplementuj klasyfikator ensemble z soft voting w Pythonie. Porównaj wyniki z wbudowanym modelem `VotingClassifier`.\n",
    "\n",
    "1. Wygeneruj syntetyczny zbiór danych binarnej klasyfikacji (np. za pomocą `make_classification` z scikit-learn).\n",
    "2. Zbuduj trzy różne modele bazowe (np. Logistic Regression, Decision Tree, SVM).\n",
    "3. Zaimplementuj soft voting ręcznie, obliczając średnie prawdopodobieństwa dla każdej klasy.\n",
    "4. Porównaj wyniki swojej implementacji z modelem `VotingClassifier` z scikit-learn.\n",
    "5. Oceń dokładność (accuracy) obu podejść na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "1b52bb6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:05:18.032739Z",
     "start_time": "2025-05-18T23:05:17.379666Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Dane\n",
    "X, y = make_classification(n_samples=2500, n_features=20, n_informative=15,\n",
    "                           n_redundant=5, random_state=42, n_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Modele bazowe\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf2 = DecisionTreeClassifier(random_state=42)\n",
    "clf3 = SVC(probability=True)  # SVM musi mieć probability=True dla soft voting\n",
    "\n",
    "models = [clf1, clf2, clf3]\n",
    "\n",
    "# Trening modeli bazowych\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Ręczny soft voting\n",
    "probs = [model.predict_proba(X_test) for model in models]\n",
    "avg_probs = np.mean(probs, axis=0)\n",
    "manual_preds = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "# 4. VotingClassifier z soft voting\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', clf1),\n",
    "    ('dt', clf2),\n",
    "    ('svc', clf3)\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "sklearn_preds = voting_clf.predict(X_test)\n",
    "\n",
    "# 5. Ocena\n",
    "manual_acc = accuracy_score(y_test, manual_preds)\n",
    "sklearn_acc = accuracy_score(y_test, sklearn_preds)\n",
    "\n",
    "print(f\"Manual soft voting accuracy:   {manual_acc:.4f}\")\n",
    "print(f\"VotingClassifier accuracy:     {sklearn_acc:.4f}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual soft voting accuracy:   0.9133\n",
      "VotingClassifier accuracy:     0.9133\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "3423f331",
   "metadata": {},
   "source": [
    "# Lasy Losowe\n",
    "\n",
    "## Czym są lasy losowe?\n",
    "\n",
    "Lasy losowe (ang. Random Forests) to metoda ensemble oparta na technice **bagging** (Bootstrap Aggregating), która łączy wiele drzew decyzyjnych w celu poprawy dokładności i stabilności predykcji. Została zaproponowana przez Leo Breimana w 2001 roku i jest szeroko stosowana zarówno w zadaniach klasyfikacji, jak i regresji.\n",
    "\n",
    "### Kluczowe cechy lasów losowych:\n",
    "1. **Wiele drzew decyzyjnych**: Las losowy składa się z $ T $ drzew decyzyjnych, gdzie każde drzewo jest trenowane na losowym podzbiorze danych.\n",
    "2. **Bootstrap**: Każde drzewo jest trenowane na losowym podzbiorze danych treningowych, wybranym z powtórzeniami (tzw. bootstrap sample). Zazwyczaj wielkość podzbioru jest równa wielkości oryginalnego zbioru danych.\n",
    "3. **Losowy wybór cech**: W każdym węźle drzewa decyzyjnego wybierana jest losowa podgrupa cech (atrybutów), spośród których wybierana jest najlepsza cecha do podziału. To zmniejsza korelację między drzewami i zwiększa różnorodność.\n",
    "4. **Agregacja wyników**:\n",
    "   - Dla **klasyfikacji**: Wyniki drzew są łączone przez głosowanie większościowe (ang. majority voting).\n",
    "   - Dla **regresji**: Wyniki drzew są uśredniane.\n",
    "\n",
    "### Formalny opis matematyczny\n",
    "\n",
    "Załóżmy, że mamy zbiór danych treningowych $ D = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)\\} $, gdzie $ x_i $ to wektor cech, a $ y_i $ to etykieta (dla klasyfikacji) lub wartość ciągła (dla regresji).\n",
    "\n",
    "#### Krok 1: Tworzenie podzbiorów danych\n",
    "Dla każdego drzewa $ t = 1, 2, \\dots, T $:\n",
    "- Losujemy z powtórzeniami podzbiór danych $ D_t $ o rozmiarze $ N $ (bootstrap sample).\n",
    "- Losowy podzbiór cech (np. $ \\sqrt{p} $ lub $ p/3 $, gdzie $ p $ to liczba wszystkich cech) jest wybierany w każdym węźle drzewa do określenia najlepszego podziału.\n",
    "\n",
    "#### Krok 2: Budowa drzew\n",
    "Każde drzewo $ h_t(x) $ jest trenowane na zbiorze $ D_t $. Drzewo podejmuje decyzje na podstawie reguł podziału opartych na wybranych cechach. W przeciwieństwie do pojedynczego drzewa decyzyjnego, las losowy ogranicza overfitting dzięki różnorodności drzew.\n",
    "\n",
    "#### Krok 3: Agregacja predykcji\n",
    "- Dla **klasyfikacji**:\n",
    "  Predykcja lasu losowego to klasa, która otrzymała najwięcej głosów od drzew:\n",
    "  $$\n",
    "  \\hat{y}(x) = \\text{mode} \\{ h_1(x), h_2(x), \\dots, h_T(x) \\},\n",
    "  $$\n",
    "  gdzie $ \\text{mode} $ oznacza najczęściej występującą klasę.\n",
    "- Dla **regresji**:\n",
    "  Predykcja to średnia predykcji wszystkich drzew:\n",
    "  $$\n",
    "  \\hat{y}(x) = \\frac{1}{T} \\sum_{t=1}^T h_t(x).\n",
    "  $$\n",
    "\n",
    "#### Krok 4: Ważność cech (Feature Importance)\n",
    "Lasy losowe mogą oceniać ważność cech na podstawie tego, jak bardzo każda cecha przyczynia się do zmniejszenia nieczystości (np. entropii lub współczynnika Gini) w węzłach drzew. Ważność cechy $ j $ można zapisać jako:\n",
    "$$\n",
    "\\text{Importance}(j) = \\frac{1}{T} \\sum_{t=1}^T \\sum_{\\text{węzły } v \\text{ w drzewie } t} \\Delta i(v, j),\n",
    "$$\n",
    "gdzie $ \\Delta i(v, j) $ to zmniejszenie nieczystości w węźle $ v $ dzięki cesze $ j $.\n",
    "\n",
    "### Zalety lasów losowych\n",
    "- Odporność na overfitting dzięki agregacji wielu drzew.\n",
    "- Dobra wydajność w zadaniach z dużą liczbą cech i szumem.\n",
    "- Możliwość oceny ważności cech.\n",
    "- Prosta implementacja i równoległe trenowanie drzew.\n",
    "\n",
    "### Wady\n",
    "- Wysoka złożoność obliczeniowa przy dużej liczbie drzew i danych.\n",
    "- Mniejsza interpretowalność w porównaniu do pojedynczego drzewa decyzyjnego.\n",
    "- Może być mniej skuteczny w zadaniach wymagających modelowania złożonych zależności (np. w porównaniu do gradient boosting).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012146a",
   "metadata": {},
   "source": [
    "### Zadanie nr 2 (3 punkty)\n",
    "Zbuduj model lasów losowych w Pythonie, używając scikit-learn, i oceń jego dokładność na syntetycznym zbiorze danych klasyfikacji binarnej.\n",
    "\n",
    "1. Wygeneruj zbiór danych binarnej klasyfikacji (użyj `make_classification`).\n",
    "2. Podziel dane na zbiór treningowy i testowy.\n",
    "3. Zbuduj model lasów losowych z 50 drzewami.\n",
    "4. Oceń dokładność modelu na zbiorze testowym."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "646153d460f0ceca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:06:47.841611Z",
     "start_time": "2025-05-18T23:06:46.327765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Wygeneruj dane\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=100,\n",
    "    n_informative=60,\n",
    "    n_redundant=20,\n",
    "    n_classes=10,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Podział danych\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Random Forest z 50 drzewami\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Ocena\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Dokładność (accuracy): {acc:.4f}\\n\")\n",
    "print(\"Raport klasyfikacji:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "c9e2b69085166a06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność (accuracy): 0.8807\n",
      "\n",
      "Raport klasyfikacji:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84       122\n",
      "           1       0.91      0.85      0.88       172\n",
      "           2       0.91      0.90      0.90       162\n",
      "           3       0.86      0.88      0.87       138\n",
      "           4       0.85      0.89      0.87       142\n",
      "           5       0.91      0.89      0.90       145\n",
      "           6       0.86      0.84      0.85       154\n",
      "           7       0.94      0.87      0.90       152\n",
      "           8       0.94      0.85      0.89       173\n",
      "           9       0.88      0.92      0.90       140\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "91011b97",
   "metadata": {},
   "source": [
    "# Algorytm Gradient Boosting dla Klasyfikacji (Binary)\n",
    "\n",
    "## Dane wejściowe\n",
    "\n",
    "- Zbiór uczący: $ (x_1, y_1), \\dots, (x_n, y_n) $, gdzie $ y_i \\in \\{0, 1\\} $\n",
    "- Liczba iteracji (drzew): $ T $\n",
    "- Learning rate: $ \\eta \\in (0, 1] $\n",
    "- Funkcja straty: log-loss\n",
    "\n",
    "---\n",
    "\n",
    "## Algorytm\n",
    "\n",
    "### 1. Inicjalizacja\n",
    "\n",
    "Oblicz początkową wartość predykcji jako logit średniego udziału klasy pozytywnej:\n",
    "\n",
    "$$\n",
    "\\hat{y}_0 = \\log \\left( \\frac{p}{1 - p} \\right), \\quad \\text{gdzie } p = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dla każdej iteracji $ t = 1 \\dots T $:\n",
    "\n",
    "#### a) Oblicz prawdopodobieństwa\n",
    "\n",
    "$$\n",
    "p_i^{(t-1)} = \\frac{1}{1 + e^{-\\hat{y}_{t-1}(x_i)}}\n",
    "$$\n",
    "\n",
    "#### b) Oblicz pseudoreszty (gradient log-loss)\n",
    "\n",
    "$$\n",
    "r_i^{(t)} = y_i - p_i^{(t-1)}\n",
    "$$\n",
    "\n",
    "#### c) Naucz drzewo regresyjne $ h_t(x) $, dopasowujące się do $ (x_i, r_i^{(t)}) $\n",
    "\n",
    "#### d) Zaktualizuj predykcję\n",
    "\n",
    "$$\n",
    "\\hat{y}_t(x) = \\hat{y}_{t-1}(x) + \\eta \\cdot h_t(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Końcowa predykcja\n",
    "\n",
    "#### a) Prawdopodobieństwo klasy 1:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid x) = \\frac{1}{1 + e^{-\\hat{y}_T(x)}}\n",
    "$$\n",
    "\n",
    "#### b) Klasa końcowa:\n",
    "\n",
    "$$\n",
    "\\text{klasa}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{jeśli } P(y=1 \\mid x) \\geq 0{,}5 \\\\\n",
    "0 & \\text{w przeciwnym razie}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Uwagi\n",
    "\n",
    "- Dla klasyfikacji wieloklasowej używa się softmax zamiast sigmoid.\n",
    "- Popularne implementacje: `XGBoost`, `LightGBM`, `CatBoost`, `sklearn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf773d",
   "metadata": {},
   "source": [
    "### Zadanie nr 3 (1 punkt)\n",
    "Zbuduj model Gradient Boosting w Pythonie na danych `make_moons` i oceń jego dokładność.\n",
    "\n",
    "### Instrukcje\n",
    "1. Wygeneruj zbiór danych `make_moons` (500 próbek, szum 0.2).\n",
    "2. Podziel dane na treningowe i testowe (30% testowe).\n",
    "3. Zbuduj model Gradient Boosting z 100 drzewami i learning rate 0.1.\n",
    "4. Wypisz dokładność na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "67e57bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:09:38.216971Z",
     "start_time": "2025-05-18T23:09:38.098533Z"
    }
   },
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Wygeneruj dane\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# 2. Podział na zbiór treningowy/testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Gradient Boosting z 100 drzewami i learning_rate=0.1\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Ocena dokładności\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Dokładność na zbiorze testowym: {acc:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność na zbiorze testowym: 0.9733\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "80a8188a",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "\n",
    "Random Search (ang. losowe przeszukiwanie) to metoda strojenia hiperparametrów modelu uczenia maszynowego, polegająca na losowym próbkowaniu kombinacji wartości hiperparametrów z zadanego zakresu, zamiast testowania wszystkich możliwych kombinacji (jak w Grid Search). Dla każdej losowej kombinacji model jest trenowany i oceniany (np. za pomocą walidacji krzyżowej), a najlepsza kombinacja jest wybierana.\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Losowość**: Hiperparametry są losowane z zdefiniowanych rozkładów (np. jednorodnego, logarytmicznego).\n",
    "- **Efektywność**: Testuje mniejszą liczbę kombinacji niż Grid Search, co przyspiesza proces dla dużych przestrzeni hiperparametrów.\n",
    "- **Elastyczność**: Pozwala na określenie zakresów ciągłych (np. learning rate od 0.001 do 0.1) lub dyskretnych (np. liczba drzew: 50, 100, 200).\n",
    "\n",
    "### Matematyka:\n",
    "Niech $ \\Theta $ to przestrzeń hiperparametrów (np. $\\Theta = \\{ \\text{n\\_estimators}, \\text{learning\\_rate}, \\text{max\\_depth} \\}$). Random Search losuje $ n $ kombinacji $ \\theta_1, \\theta_2, \\dots, \\theta_n \\in \\Theta $ z rozkładu (np. jednorodnego) i ocenia model dla każdej $ \\theta_i $ za pomocą funkcji straty $ L(\\theta_i) $ (np. średni błąd walidacji krzyżowej). Wybiera:\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta_i} L(\\theta_i).\n",
    "$$\n",
    "\n",
    "### Dlaczego Random Search jest ważny?\n",
    "- **Szybsze niż Grid Search**: W dużych przestrzeniach hiperparametrów testuje mniej kombinacji, zachowując wysoką szansę na znalezienie dobrych wartości.\n",
    "- **Skuteczność w wysokich wymiarach**: Losowe próbkowanie lepiej eksploruje przestrzeń niż systematyczne grid search, zwłaszcza gdy tylko niektóre hiperparametry mają duże znaczenie.\n",
    "- **Praktyczność**: Umożliwia strojenie modeli w rozsądnym czasie, co jest kluczowe w rzeczywistych zastosowaniach, gdzie zasoby obliczeniowe są ograniczone.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703249be",
   "metadata": {},
   "source": [
    "### Zadanie nr 4 (1 punkt)\n",
    "Użyj Random Search do strojenia hiperparametrów modelu XGBoost na danych `make_moons`, testując większą liczbę hiperparametrów, i wypisz najlepszą dokładność oraz wybrane hiperparametry.\n",
    "\n",
    "### Instrukcje\n",
    "1. Wygeneruj zbiór danych `make_moons` (500 próbek, szum 0.2).\n",
    "2. Podziel dane na treningowe i testowe (30% testowe).\n",
    "3. Użyj `RandomizedSearchCV`, aby znaleźć najlepsze hiperparametry dla XGBoost, testując: `n_estimators`, `learning_rate`, `max_depth`, `min_child_weight`, `subsample`.\n",
    "4. Wypisz najlepszą dokładność i najlepsze hiperparametry."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:21:53.786461Z",
     "start_time": "2025-05-18T23:21:53.781942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import get_tags\n",
    "\n",
    "model = XGBClassifier()\n",
    "print(get_tags(model))\n"
   ],
   "id": "2be24e10daa8fb85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags(estimator_type='classifier', target_tags=TargetTags(required=True, one_d_labels=False, two_d_labels=False, positive_only=False, multi_output=False, single_output=True), transformer_tags=None, classifier_tags=ClassifierTags(poor_score=False, multi_class=True, multi_label=True), regressor_tags=None, array_api_support=False, no_validation=True, non_deterministic=False, requires_fit=True, _skip_test=False, input_tags=InputTags(one_d_array=False, two_d_array=True, three_d_array=False, sparse=True, categorical=False, string=False, dict=False, positive_only=False, allow_nan=True, pairwise=False))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:21:55.444528Z",
     "start_time": "2025-05-18T23:21:55.440528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost\n",
    "print(xgboost.__file__)\n",
    "print(xgboost.__version__)\n"
   ],
   "id": "e50aba30dd6a41f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msztu223/miniconda3/envs/PSI/lib/python3.10/site-packages/xgboost/__init__.py\n",
      "3.0.1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "24133f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T23:23:09.583352Z",
     "start_time": "2025-05-18T23:23:08.470114Z"
    }
   },
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Dane\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Parametry do przeszukania\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Klasyfikator\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Wyniki\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Najlepsza dokładność:\", acc)\n",
    "print(\"Najlepsze hiperparametry:\", random_search.best_params_)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Najlepsza dokładność: 0.9533333333333334\n",
      "Najlepsze hiperparametry: {'subsample': 0.7, 'n_estimators': 50, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
